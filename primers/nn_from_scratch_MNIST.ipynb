{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "Simply implementing a Feed-Forward Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.datasets import mnist # cheating a little, loading easy mnist dataset from keras library\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit # more robust sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 784\n",
      "Output: 10\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize Pixel Values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert array of ints (digit values) to one-hot encoded categorical\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# convert images from 28x28 to 1x784\n",
    "X_train = np.reshape(X_train, (60000, 784))\n",
    "X_test = np.reshape(X_test, (10000, 784))\n",
    "\n",
    "print(\"Input:\", X_train.shape[1])\n",
    "print(\"Output:\", y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions\n",
    "\n",
    "# Activation Function\n",
    "def activation(x, derivative=False):\n",
    "    return relu(x, derivative)\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    return 1 - np.power(x, 2) if derivative else np.tanh(x)\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    return x * (1 - x) if derivative else expit(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    return (x>0).astype(x.dtype) if derivative else np.maximum(x, 0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.apply_along_axis(_softmax, 1, x)\n",
    "\n",
    "def _softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)    \n",
    "\n",
    "\n",
    "def calculate_loss(model): \n",
    "    W1, b1, W2, b2= model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation train to calculate our predictions \n",
    "    l1 = activation(X_train.dot(W1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias) \n",
    "    output_train = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "    \n",
    "    # Forward propagation train to calculate our predictions \n",
    "    l1 = activation(X_test.dot(W1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias) \n",
    "    output_test = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "    \n",
    "    # Calculating the loss\n",
    "    return log_loss(y_train, output_train), log_loss(y_test, output_test)\n",
    "\n",
    "\n",
    "def get_mini_batches(X, y, batch_size):\n",
    "    random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "    X_shuffled = X[random_idxs,:]\n",
    "    y_shuffled = y[random_idxs]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "\n",
    "# Layer Parameters\n",
    "num_examples = X_train.shape[0] # training set size (60000)\n",
    "nn_input_dim = X_train.shape[1] # input layer dimensionality (784)\n",
    "nn_hdim_1 = 15\n",
    "nn_output_dim = y_train.shape[1] # output layer dimensionality (10)\n",
    "\n",
    "# Gradient descent parameters\n",
    "epochs = 700 # How many times be forward and back propigate the network\n",
    "epsilon = .001 # learning rate for gradient descent\n",
    "reg_lambda = 0 # regularization strength\n",
    "mu = 0.001  # momentum constant (mu in [0, 1])\n",
    "batch_size = 128 # size of batches for minibatch gradient descent\n",
    "print_loss = 10  #prints loss (and checks for early stopping) every 10 epochs\n",
    "stop_threshold = 0.00001  # threshold for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 15)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input -> Hidden 1\n",
    "W1 = np.random.randn(nn_input_dim, nn_hdim_1).astype(np.float32)\n",
    "v1 = np.zeros_like(W1)  # to track previous W1 gradients for momentum\n",
    "b1 = np.zeros((1, nn_hdim_1))\n",
    "\n",
    "# Hidden 1 -> Output\n",
    "W2 = np.random.randn(nn_hdim_1, nn_output_dim).astype(np.float32)\n",
    "v2 = np.zeros_like(W2)  # to track previous W2 gradients for momentum\n",
    "b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "W1.shape # for each node in input, there is a weight that corresponds with a node in hidden layer 1 (23520 total weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.163329\n",
      "Loss after iteration 10: 0.452741\n",
      "Loss after iteration 20: 0.384089\n",
      "Loss after iteration 30: 0.345098\n",
      "Loss after iteration 40: 0.309269\n",
      "Loss after iteration 50: 0.292867\n",
      "Loss after iteration 60: 0.271944\n",
      "Loss after iteration 70: 0.259494\n",
      "Loss after iteration 80: 0.260454\n",
      "Loss after iteration 90: 0.249945\n",
      "Loss after iteration 100: 0.241630\n",
      "Loss after iteration 110: 0.229690\n",
      "Loss after iteration 120: 0.229492\n",
      "Loss after iteration 130: 0.210972\n",
      "Loss after iteration 140: 0.207654\n",
      "Loss after iteration 150: 0.207518\n",
      "Loss after iteration 160: 0.208228\n",
      "Loss after iteration 170: 0.191769\n",
      "Loss after iteration 180: 0.190699\n",
      "Loss after iteration 190: 0.186778\n",
      "Loss after iteration 200: 0.184160\n",
      "Loss after iteration 210: 0.177734\n",
      "Loss after iteration 220: 0.178292\n",
      "Loss after iteration 230: 0.171745\n",
      "Loss after iteration 240: 0.168617\n",
      "Loss after iteration 250: 0.166656\n",
      "Loss after iteration 260: 0.172095\n",
      "Loss after iteration 270: 0.160377\n",
      "Loss after iteration 280: 0.159675\n",
      "Loss after iteration 290: 0.158199\n",
      "Loss after iteration 300: 0.158177\n",
      "Loss after iteration 310: 0.150925\n",
      "Loss after iteration 320: 0.148936\n",
      "Loss after iteration 330: 0.150056\n",
      "Loss after iteration 340: 0.146312\n",
      "Loss after iteration 350: 0.144193\n",
      "Loss after iteration 360: 0.145026\n",
      "Loss after iteration 370: 0.142135\n",
      "Loss after iteration 380: 0.141880\n",
      "Loss after iteration 390: 0.143860\n",
      "Loss after iteration 400: 0.141049\n",
      "Loss after iteration 410: 0.138179\n",
      "Loss after iteration 420: 0.131535\n",
      "Loss after iteration 430: 0.132280\n",
      "Loss after iteration 440: 0.132104\n",
      "Loss after iteration 450: 0.129891\n",
      "Loss after iteration 460: 0.128932\n",
      "Loss after iteration 470: 0.126946\n",
      "Loss after iteration 480: 0.128569\n",
      "Loss after iteration 490: 0.125666\n",
      "Loss after iteration 500: 0.126794\n",
      "Loss after iteration 510: 0.123104\n",
      "Loss after iteration 520: 0.125248\n",
      "Loss after iteration 530: 0.120261\n",
      "Loss after iteration 540: 0.119227\n",
      "Loss after iteration 550: 0.119326\n",
      "Loss after iteration 560: 0.118855\n",
      "Loss after iteration 570: 0.115220\n",
      "Loss after iteration 580: 0.114564\n",
      "Loss after iteration 590: 0.115806\n",
      "Loss after iteration 600: 0.113883\n",
      "Loss after iteration 610: 0.111589\n",
      "Loss after iteration 620: 0.110069\n",
      "Loss after iteration 630: 0.110324\n",
      "Loss after iteration 640: 0.110395\n",
      "Loss after iteration 650: 0.107279\n",
      "Loss after iteration 660: 0.106429\n",
      "Loss after iteration 670: 0.107500\n",
      "Loss after iteration 680: 0.110312\n"
     ]
    }
   ],
   "source": [
    "# Now our Network\n",
    "\n",
    "model = {}\n",
    "losses_log = []  #simple list to plot loss over time\n",
    "prev_loss = 10000  # init previous loss to track differences for stoping threshold\n",
    "\n",
    "# Gradient descent... \n",
    "for i in range(0, epochs):\n",
    "    \n",
    "    mini_batches = get_mini_batches(X_train, y_train, batch_size)\n",
    "    for mb in mini_batches:\n",
    "        X_mb = mb[0]\n",
    "        y_mb = mb[1]\n",
    " \n",
    "        # Forward propagation\n",
    "        l1 = activation(X_mb.dot(W1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias)\n",
    "        output = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "\n",
    "        # Backpropagation   \n",
    "        output_error = output - y_mb # technically, you'd need a derived softmax activation, but that equals 1, so we don't add it\n",
    "        l1_error = output_error.dot(W2.T) * activation(l1, True)\n",
    "    \n",
    "        dW2 = np.dot(l1.T, output_error)\n",
    "        db2 = np.average(output_error, axis=0)\n",
    "        dW1 = np.dot(X_mb.T, l1_error)\n",
    "        db1 = np.average(l1_error, axis=0)\n",
    "    \n",
    "        # add regularization terms to weights\n",
    "        dW2 += reg_lambda * W2 \n",
    "        dW1 += reg_lambda * W1\n",
    "    \n",
    "        # update velocity by taking momentum multiplied by previous gradient (velocity) plus gradient multipled by learning rate\n",
    "        v1 = mu * v1 + epsilon * dW1\n",
    "        v2 = mu * v2 + epsilon * dW2\n",
    "        \n",
    "        # Update weights\n",
    "        W1 -= v1\n",
    "        b1 += -epsilon * db1 \n",
    "        W2 -= v2\n",
    "        b2 += -epsilon * db2\n",
    "    \n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2} \n",
    "    \n",
    "    # Optionally print the loss. \n",
    "    # This is expensive because it uses the whole dataset, so we don't want to do it too often. \n",
    "    # In the future, we should use a validation set to detect overfitting.\n",
    "    if i % print_loss == 0 or i == epochs-1:\n",
    "        loss_train, loss_test = calculate_loss(model)\n",
    "        losses_log.append([i,loss_train, loss_test])\n",
    "        print(\"Loss after iteration %i: %f\" %(i, loss_train))\n",
    "        \n",
    "        # Check to see if stop early\n",
    "        if np.absolute(loss_train - prev_loss) <= stop_threshold:\n",
    "            print(\"Gradient Descent Stopped Early!\")\n",
    "            break\n",
    "        else:\n",
    "            prev_loss = loss_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot our loss over the iterations\n",
    "\n",
    "iters = []\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "for ll in losses_log:\n",
    "    iters.append(ll[0])\n",
    "    losses_train.append(ll[1])\n",
    "    losses_test.append(ll[2])\n",
    "    \n",
    "plt.plot(iters, losses_train)\n",
    "plt.plot(iters, losses_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how we did\n",
    "\n",
    "# Forward Propigate to get outputs on train data\n",
    "l1_train = activation(X_train.dot(W1) + b1) # Input -> Hidden 1 || activation(x * W + bias)\n",
    "output_train = softmax(l1_train.dot(W2) + b2)\n",
    "\n",
    "# Forward Propigate to get outputs on test data\n",
    "l1_test = activation(X_test.dot(W1) + b1) # Input -> Hidden 1 || activation(x * W + bias)\n",
    "output_test = softmax(l1_test.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "\n",
    "correct_train = 0\n",
    "for i in range(0, output_train.shape[0]):\n",
    "    if np.argmax(output_train[i]) == np.argmax(y_train[i]):\n",
    "        correct_train += 1\n",
    "\n",
    "correct_test = 0\n",
    "for i in range(0, output_test.shape[0]):\n",
    "    if np.argmax(output_test[i]) == np.argmax(y_test[i]):\n",
    "        correct_test += 1\n",
    "\n",
    "train_accuracy = correct_train / y_train.shape[0]\n",
    "test_accuracy = correct_test / y_test.shape[0]\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
