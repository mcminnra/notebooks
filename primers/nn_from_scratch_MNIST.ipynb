{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "Simply implementing a Feed-Forward Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load some libraries\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.datasets import mnist # cheating a little, loading easy mnist dataset from keras library\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.special import expit # more robust sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 784\n",
      "Output: 10\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize Pixel Values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert array of ints (digit values) to one-hot encoded categorical\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# convert images from 28x28 to 1x784\n",
    "X_train = np.reshape(X_train, (60000, 784))\n",
    "X_test = np.reshape(X_test, (10000, 784))\n",
    "\n",
    "# get validation set - essentially split test in half\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(\"Input:\", X_train.shape[1])\n",
    "print(\"Output:\", y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Useful Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions\n",
    "\n",
    "# Activation Function\n",
    "def activation(x, derivative=False):\n",
    "    return relu(x, derivative=derivative)\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    return 1 - np.power(x, 2) if derivative else np.tanh(x)\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    return x * (1 - x) if derivative else expit(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    return (x>0).astype(x.dtype) if derivative else np.maximum(x, 0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.apply_along_axis(_softmax, 1, x)\n",
    "\n",
    "def _softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)    \n",
    "\n",
    "def calculate_loss(model):\n",
    "    # Computes loss for train and validation sets\n",
    "    W1, b1, W2, b2= model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Train Forward propagation train to calculate our predictions \n",
    "    l1 = activation(X_train.dot(W1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias) \n",
    "    output_train = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "    \n",
    "    # Validation Forward propagation train to calculate our predictions \n",
    "    l1 = activation(X_val.dot(W1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias) \n",
    "    output_val = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "    \n",
    "    # Calculating the loss\n",
    "    return log_loss(y_train, output_train), log_loss(y_val, output_val)\n",
    "\n",
    "def get_mini_batches(X, y, batch_size):\n",
    "    random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "    X_shuffled = X[random_idxs,:]\n",
    "    y_shuffled = y[random_idxs]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    return mini_batches\n",
    "\n",
    "def drop_connect_mask(prob, dimensions):\n",
    "    mask_vector = np.random.binomial(1, prob, np.prod(dimensions))\n",
    "    return mask_vector.reshape(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Params and Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "\n",
    "# Layer Parameters\n",
    "num_examples = X_train.shape[0] # training set size (60000)\n",
    "nn_input_dim = X_train.shape[1] # input layer dimensionality (784)\n",
    "nn_hdim_1 = 50\n",
    "nn_output_dim = y_train.shape[1] # output layer dimensionality (10)\n",
    "\n",
    "# Gradient descent parameters\n",
    "epochs = 2000 # How many times to forward and back propigate the network\n",
    "learning_rate = .001 # learning rate for gradient descent\n",
    "decay = learning_rate / epochs # Default recommended by this blog post (https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/)\n",
    "reg_lambda = 0 # regularization strength\n",
    "drop_connect_prob = .3 # Probability to drop weight for DropConnect\n",
    "batch_size = 128 # size of batches for minibatch gradient descent\n",
    "print_loss = 20  # prints loss (and checks for early stopping) in some number of epochs\n",
    "stop_threshold = 4  # threshold for early stopping using generalization error\n",
    "\n",
    "# Constants for Adam Optimization (values are paper recommended values)\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1E-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input -> Hidden 1\n",
    "W1 = np.random.randn(nn_input_dim, nn_hdim_1).astype(np.float32)\n",
    "mW1 = np.zeros_like(W1)  # first-moment vector Adam Optimziation for W1\n",
    "vW1 = np.zeros_like(W1)  # second-moment vector Adam Optimization for W1\n",
    "b1 = np.zeros((1, nn_hdim_1))\n",
    "\n",
    "# Hidden 1 -> Output\n",
    "W2 = np.random.randn(nn_hdim_1, nn_output_dim).astype(np.float32)\n",
    "mW2 = np.zeros_like(W2)  # first-moment vector Adam Optimziation for W2\n",
    "vW2 = np.zeros_like(W2)  # second-moment vector Adam Optimization for W2\n",
    "b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "W1.shape # for each node in input, there is a weight that corresponds with a node in hidden layer 1 (23520 total weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 1 -> Train: 11.928844, Validation: 11.704974\n",
      "\t- Generalization Loss: -99.882950\n",
      "Loss after iteration 20 -> Train: 2.069785, Validation: 2.058670\n",
      "\t- Generalization Loss: -82.412011\n",
      "Loss after iteration 40 -> Train: 0.905545, Validation: 0.910487\n",
      "\t- Generalization Loss: -55.773029\n",
      "Loss after iteration 60 -> Train: 0.688249, Validation: 0.716971\n",
      "\t- Generalization Loss: -21.254140\n",
      "Loss after iteration 80 -> Train: 0.601127, Validation: 0.636245\n",
      "\t- Generalization Loss: -11.259285\n",
      "Loss after iteration 100 -> Train: 0.519188, Validation: 0.568494\n",
      "\t- Generalization Loss: -10.648600\n",
      "Loss after iteration 120 -> Train: 0.458394, Validation: 0.512845\n",
      "\t- Generalization Loss: -9.788760\n",
      "Loss after iteration 140 -> Train: 0.402886, Validation: 0.451076\n",
      "\t- Generalization Loss: -12.044551\n",
      "Loss after iteration 160 -> Train: 0.365716, Validation: 0.413095\n",
      "\t- Generalization Loss: -8.419913\n",
      "Loss after iteration 180 -> Train: 0.332083, Validation: 0.380740\n",
      "\t- Generalization Loss: -7.832306\n",
      "Loss after iteration 200 -> Train: 0.300135, Validation: 0.351376\n",
      "\t- Generalization Loss: -7.712523\n",
      "Loss after iteration 220 -> Train: 0.280677, Validation: 0.335585\n",
      "\t- Generalization Loss: -4.494080\n",
      "Loss after iteration 240 -> Train: 0.263298, Validation: 0.318692\n",
      "\t- Generalization Loss: -5.033740\n",
      "Loss after iteration 260 -> Train: 0.261114, Validation: 0.318015\n",
      "\t- Generalization Loss: -0.212523\n",
      "Loss after iteration 280 -> Train: 0.237311, Validation: 0.295433\n",
      "\t- Generalization Loss: -7.100784\n",
      "Loss after iteration 300 -> Train: 0.226394, Validation: 0.287257\n",
      "\t- Generalization Loss: -2.767564\n",
      "Loss after iteration 320 -> Train: 0.216202, Validation: 0.274115\n",
      "\t- Generalization Loss: -4.575008\n",
      "Loss after iteration 340 -> Train: 0.212357, Validation: 0.277210\n",
      "\t- Generalization Loss: 1.129077\n",
      "Loss after iteration 360 -> Train: 0.200373, Validation: 0.260108\n",
      "\t- Generalization Loss: -5.109866\n",
      "Loss after iteration 380 -> Train: 0.192705, Validation: 0.258081\n",
      "\t- Generalization Loss: -0.779284\n",
      "Loss after iteration 400 -> Train: 0.201496, Validation: 0.263840\n",
      "\t- Generalization Loss: 2.231375\n",
      "Loss after iteration 420 -> Train: 0.190878, Validation: 0.255355\n",
      "\t- Generalization Loss: -1.056465\n",
      "Loss after iteration 440 -> Train: 0.178580, Validation: 0.252815\n",
      "\t- Generalization Loss: -0.994643\n",
      "Loss after iteration 460 -> Train: 0.182912, Validation: 0.250454\n",
      "\t- Generalization Loss: -0.933923\n",
      "Loss after iteration 480 -> Train: 0.178165, Validation: 0.251084\n",
      "\t- Generalization Loss: 0.251892\n",
      "Loss after iteration 500 -> Train: 0.170109, Validation: 0.245460\n",
      "\t- Generalization Loss: -1.994005\n",
      "Loss after iteration 520 -> Train: 0.170779, Validation: 0.246922\n",
      "\t- Generalization Loss: 0.595650\n",
      "Loss after iteration 540 -> Train: 0.164235, Validation: 0.241951\n",
      "\t- Generalization Loss: -1.429274\n",
      "Loss after iteration 560 -> Train: 0.169193, Validation: 0.246141\n",
      "\t- Generalization Loss: 1.731577\n",
      "Loss after iteration 580 -> Train: 0.167369, Validation: 0.243013\n",
      "\t- Generalization Loss: 0.438755\n",
      "Loss after iteration 600 -> Train: 0.162735, Validation: 0.241518\n",
      "\t- Generalization Loss: -0.178966\n",
      "Loss after iteration 620 -> Train: 0.155072, Validation: 0.236901\n",
      "\t- Generalization Loss: -1.911585\n",
      "Loss after iteration 640 -> Train: 0.156007, Validation: 0.232652\n",
      "\t- Generalization Loss: -1.793741\n",
      "Loss after iteration 660 -> Train: 0.151440, Validation: 0.230410\n",
      "\t- Generalization Loss: -0.963670\n",
      "Loss after iteration 680 -> Train: 0.154951, Validation: 0.239610\n",
      "\t- Generalization Loss: 3.992921\n",
      "Loss after iteration 700 -> Train: 0.148885, Validation: 0.236474\n",
      "\t- Generalization Loss: 2.631709\n",
      "Loss after iteration 720 -> Train: 0.148566, Validation: 0.233530\n",
      "\t- Generalization Loss: 1.353897\n",
      "Loss after iteration 740 -> Train: 0.153398, Validation: 0.238222\n",
      "\t- Generalization Loss: 3.390461\n",
      "Loss after iteration 760 -> Train: 0.146811, Validation: 0.232458\n",
      "\t- Generalization Loss: 0.888891\n",
      "Loss after iteration 780 -> Train: 0.148059, Validation: 0.238688\n",
      "\t- Generalization Loss: 3.592742\n",
      "Loss after iteration 800 -> Train: 0.141406, Validation: 0.230166\n",
      "\t- Generalization Loss: -0.106002\n",
      "Loss after iteration 820 -> Train: 0.145600, Validation: 0.233780\n",
      "\t- Generalization Loss: 1.570304\n",
      "Loss after iteration 840 -> Train: 0.143400, Validation: 0.234539\n",
      "\t- Generalization Loss: 1.900169\n",
      "Loss after iteration 860 -> Train: 0.140094, Validation: 0.229279\n",
      "\t- Generalization Loss: -0.385480\n",
      "Loss after iteration 880 -> Train: 0.139489, Validation: 0.231324\n",
      "\t- Generalization Loss: 0.892159\n",
      "Loss after iteration 900 -> Train: 0.134298, Validation: 0.225601\n",
      "\t- Generalization Loss: -1.603846\n",
      "Loss after iteration 920 -> Train: 0.136023, Validation: 0.227371\n",
      "\t- Generalization Loss: 0.784270\n",
      "Loss after iteration 940 -> Train: 0.139229, Validation: 0.229229\n",
      "\t- Generalization Loss: 1.608025\n",
      "Loss after iteration 960 -> Train: 0.135628, Validation: 0.228803\n",
      "\t- Generalization Loss: 1.419351\n",
      "Loss after iteration 980 -> Train: 0.139621, Validation: 0.238539\n",
      "\t- Generalization Loss: 5.734819\n",
      "Gradient Descent Stopped Early!\n",
      "--- Time: 4105.629874706268 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Now our Network\n",
    "\n",
    "model = {}\n",
    "losses_log = []  #simple list to plot loss over time\n",
    "opt_loss_val = 10000  # init optimum validation set loss to track generalization loss for stopping threshold\n",
    "start_time = time.time()  # track how long gradient descent takes\n",
    "\n",
    "# Gradient descent...\n",
    "# start at 1 instead of 0 for Adam Optimizer - avoids divide by 0\n",
    "for i in range(1, epochs+1):\n",
    "    \n",
    "    mini_batches = get_mini_batches(X_train, y_train, batch_size)\n",
    "    for mb in mini_batches:\n",
    "        X_mb = mb[0]\n",
    "        y_mb = mb[1]\n",
    " \n",
    "        # Forward propagation\n",
    "        # Get DropConnect Mask (0's and 1's in shape of W1)\n",
    "        # This randomly turns weights on and off with some probability\n",
    "        # http://www.matthewzeiler.com/wp-content/uploads/2017/07/icml2013.pdf\n",
    "        dc_mask = drop_connect_mask(drop_connect_prob, W1.shape)\n",
    "        dcW1 = W1 * dc_mask # Apply DropConnect Mask to W1\n",
    "        l1 = activation(X_mb.dot(dcW1) + b1) # Input -> Hidden 1 || activation(x.t * W + bias)\n",
    "        output = softmax(l1.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "\n",
    "        # Backpropagation   \n",
    "        output_error = output - y_mb # technically, you'd need a derived softmax activation, but that equals 1, so we don't add it\n",
    "        l1_error = output_error.dot(W2.T) * activation(l1, True)\n",
    "    \n",
    "        dW2 = np.dot(l1.T, output_error)\n",
    "        db2 = np.average(output_error, axis=0)\n",
    "        dW1 = np.dot(X_mb.T, l1_error)\n",
    "        db1 = np.average(l1_error, axis=0)\n",
    "    \n",
    "        # Apply DropConnect Mask to gradients, so only \"activated\" weights are updated\n",
    "        dW1 = dW1 * dc_mask\n",
    "    \n",
    "        # add regularization terms to weights\n",
    "        dW2 += reg_lambda * W2 \n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        # Update weights by using Adam Optimization (as opposed to simply learning_rate * gradient)\n",
    "        # https://arxiv.org/pdf/1412.6980.pdf\n",
    "        # http://cs231n.github.io/neural-networks-3/ (See Section: Per-parameter adaptive learning rate methods)\n",
    "        # Update W1\n",
    "        mW1 = beta1*mW1 + (1-beta1)*dW1\n",
    "        mtW1 = mW1 / (1-beta1**i)\n",
    "        vW1 = beta2*vW1 + (1-beta2)*(dW1**2)\n",
    "        vtW1 = vW1 / (1-beta2**i)\n",
    "        W1 += -learning_rate * mtW1 / (np.sqrt(vtW1) + eps)\n",
    "        \n",
    "        # Update W2\n",
    "        mW2 = beta1*mW2 + (1-beta1)*dW2\n",
    "        mtW2 = mW2 / (1-beta1**i)\n",
    "        vW2 = beta2*vW2 + (1-beta2)*(dW2**2)\n",
    "        vtW2 = vW2 / (1-beta2**i)\n",
    "        W2 += -learning_rate * mtW2 / (np.sqrt(vtW2) + eps)\n",
    "\n",
    "        # Update Biases\n",
    "        b1 += -learning_rate * db1 \n",
    "        b2 += -learning_rate * db2\n",
    "    \n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "    # Update Learning Rate\n",
    "    # Theoretically, learning rate decay is already guaranteed by Adam optimizer\n",
    "    # Uncomment this line to use a time-based learning rate decay anyways\n",
    "    #Learning_rate = Learning_rate * 1/(1 + decay * i)\n",
    "    \n",
    "    # Optionally print the loss. \n",
    "    # This is expensive because it uses the whole dataset, so we don't want to do it too often. \n",
    "    if i % print_loss == 0 or i == 1:\n",
    "        loss_train, loss_val = calculate_loss(model)\n",
    "        print(\"Loss after iteration %i -> Train: %f, Validation: %f\" %(i, loss_train, loss_val))\n",
    "        \n",
    "        # Generalization Loss Early Stopping\n",
    "        # This is intuitively \"the percent of generalization we have lost\"\n",
    "        # Section 2.1: http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf\n",
    "        generalization_loss = 100 * (loss_val/opt_loss_val - 1)\n",
    "        print(\"\\t- Generalization Loss: %f\" %(generalization_loss))\n",
    "        \n",
    "        losses_log.append([i,loss_train, loss_val, generalization_loss])\n",
    "        \n",
    "        # Check to see if stop early\n",
    "        if generalization_loss > stop_threshold:\n",
    "            print(\"Gradient Descent Stopped Early!\")\n",
    "            break\n",
    "        else:\n",
    "            # update optimal validation loss if better than current optimal\n",
    "            if loss_val < opt_loss_val :\n",
    "                opt_loss_val = loss_val\n",
    "\n",
    "print(\"--- Time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab our losses\n",
    "\n",
    "iters = []\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "gen_loss = []\n",
    "for ll in losses_log:\n",
    "    iters.append(ll[0])\n",
    "    losses_train.append(ll[1])\n",
    "    losses_val.append(ll[2])\n",
    "    gen_loss.append(ll[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcHWWd9/3P7+zdnZCdJYSbRLbsm81OEAYGAZEIZlgGkEXF4XGUUdGJjjMwPvq8mLm5EdQRZRQcZiDozSLcEEBuBFkcgSRAWBIMhmA2SCdAlu70WX/PH1Wnc7rTnXQnfbq6+3zfr1e9Tp06VXVddar7fM91VZ0qc3dERKS2xaKugIiIRE9hICIiCgMREVEYiIgICgMREUFhICIiKAxkgDGzy8zs2ajrMViY2UVm9puo6yHRUxjIXjOzVWZ2atT1GGjM7CQzW1Px/Ckz+1wVyxtvZm5mifI0d7/T3U+rVpkycCgMRPpA5QdwFcuIV7sMGbwUBlJVZvZ5M3vLzN43swfNbGzFa6eZ2ZtmttnMfmxmv+vpN2MzO87MXgzX8aKZHVfx2mVmttLMtprZ22Z2UTj90LCszWa20cx+2cW6y9+krzSzdWa23syuqXg9ZmbzzexPZrbJzH5lZiM7LPtZM/sz8NvdbMf3gDnAj8xsm5n9KJw+0cweD9+/N83svIplfmFmt5jZQjNrBk42s0+Y2UtmtsXMVpvZdRXFPB0+fhiWcWzHbrfdvJ9Pmdn/a2bPhe/pb8xs9G52kQwU7q5Bw14NwCrg1E6m/wWwEZgNpIEfAk+Hr40GtgDnAgngaiAPfG43ZV0GPBuOjwQ+AC4J13Fh+HwU0BCu/4hw3gOAKeH4AuAfCL4MZYATuihrPODh/A3ANKCpvK1hnf8AjAu376fAgg7L3hEuW9fJ+k8C1lQ8f6py+8PlVgOXh9s3K3w/J4ev/wLYDBxfsS0nhfWMAdOB94BPdahToqfvZ0X9/gQcDtSFz6+P+u9PQ+8MahlINV0E3ObuS9w9C3wTONbMxgNnAq+7+33uXgB+ALzbw/V/Aljh7v/p7gV3XwAsBz4Zvl4CpppZnbuvd/fXw+l54GBgrLu3uvvuDkj/s7s3u/urwO0EH5IAfwP8g7uvCbfvOmBehy6h68Jlt/dw2wDOAla5++3h9r0E3Av8VcU8D7j7c+5eCrflKXd/NXy+lCDIPtbN8nb3fgLc7u5/DLfnV8DMPdgu6YcUBlJNY4F3yk/cfRuwCTgwfG11xWsOrOm4gp6sP/QOcKC7NwPnE3xgrzezh81sYjjPNwADXjCz183sit2Us7pi/J2wXAgC5X4z+9DMPgSWAUVgvy6W7amDgaPL6w/LuAjYv6v1m9nRZvakmTWZ2WaC7e9uV06X72fF88rAbgGGdHPd0s8pDKSa1hF8oAFgZg0EXThrgfUE3Svl16zy+Z6sP/Q/wvXj7o+5+18SdBEtB/49nP6uu3/e3ccCXwB+bGaH7qKcgzqsf104vho4w92HVwwZd19bMX9PLgvccd7VwO86rH+Iu1+1i2XuAh4EDnL3YcBPCIKvO3XZ5fspg5vCQHpL0swyFUOCoIvicjObaWZp4P8Dnnf3VcDDwDQz+1Q47xdp/423OxYCh5vZX5tZwszOByYDD5nZfmY2NwygLLCNoNsIM/srMysHzwcEH5KlXZTzj2ZWb2ZTCPrvywecfwJ8z8wODtc7xszm9nAbKr0HfKTi+UPh9l1iZslwONLMJu1iHUOB99291cyOAv664rUmgu38SKdL7uL93OMtkgFDYSC9ZSGwvWK4zt3/L/CPBP3c64FDgAsA3H0jQd/3vxJ0HU0GFhF8cHeLu28i6Ff/WriObwBnheuOAV8l+Lb7PkG/efkb9ZHA82a2jeBb9NXuvnIXRf0OeAt4ArjB3cs/0ro5XP43ZraV4GDy0d2tfyduJjjm8IGZ/cDdtwKnEbxn6wi6aP6F4GB1V/4f4Dthff6JoF8fAHdvAb4HPBd2Ox1TueBu3k8Z5CzoqhWJlpnFCI4ZXOTuT0ZdHwhODwXeBpLhQW6RQUstA4mMmX3czIaHXUjfIujb/kPE1RKpSQoDidKxBOetbyQ4ffFT7r7dzH4S/iiq4/CTaKsrMnipm0hERNQyEBGR4CfnA8Lo0aN9/PjxUVdDRGRAWbx48UZ3H7O7+QZMGIwfP55FixZFXQ0RkQHFzDr+qrxT6iYSERGFgYiIKAxERIQBdMxARKovn8+zZs0aWltbo66K9FAmk2HcuHEkk8k9Wr6qYWBmtxFc62SDu08Np/1Pgh8Y5Qh+cHS5u39YzXqISPesWbOGoUOHMn78eIILycpA4O5s2rSJNWvWMGHChD1aR7W7iX4BnN5h2uPAVHefDvyR4IYnItIPtLa2MmrUKAXBAGNmjBo1aq9adFUNA3d/muCKkZXTflNx0a/yLQNFpJ9QEAxMe7vfoj6AfAXwSFcvhjciX2Rmi5qamvaogAdfWcd//H7VHlZPRKQ2RBYGZvYPQAG4s6t53P1Wd29098YxY3b7A7pOPfLqev7rD936zYWIRGzTpk3MnDmTmTNnsv/++3PggQe2Pc/lct1ax+WXX86bb765y3n+7d/+jTvv7PKjp0dOOOEEXn755V5ZV5QiOZvIzC4jOLB8ilf5SnmZZJzt+WI1ixCRXjJq1Ki2D9brrruOIUOGcM0117Sbx91xd2Kxzr/L3n777bst54tf/OLeV3aQ6fOWgZmdTnAHpbPDOy9VVSYZozW/qzsaikh/99ZbbzF58mQuuugipkyZwvr167nyyitpbGxkypQpfOc732mbt/xNvVAoMHz4cObPn8+MGTM49thj2bBhAwDf/va3uemmm9rmnz9/PkcddRRHHHEEv//97wFobm7m05/+NJMnT2bevHk0NjZ2uwWwfft2Lr30UqZNm8bs2bN5+umnAXj11Vc58sgjmTlzJtOnT2flypVs3bqVM844gxkzZjB16lTuueee3nzruq3ap5YuAE4CRpvZGuBagrOH0sDj4QGPP7j731SrDmds+BnH5FcBp1arCJFB6Z//z+u8sW5Lr65z8th9uPaTU/Zo2eXLl3PHHXfQ2NgIwPXXX8/IkSMpFAqcfPLJzJs3j8mTJ7dbZvPmzXzsYx/j+uuv56tf/Sq33XYb8+fP32nd7s4LL7zAgw8+yHe+8x0effRRfvjDH7L//vtz77338sorrzB79uxu1/UHP/gB6XSaV199lddff50zzzyTFStW8OMf/5hrrrmG888/n2w2i7vzwAMPMH78eB555JG2Okeh2mcTXejuB7h70t3HufvP3f1Qdz/I3WeGQ9WCAGBMbi1TWVHNIkSkDxxyyCFtQQCwYMECZs+ezezZs1m2bBlvvPHGTsvU1dVxxhlnAPDRj36UVatWdbruc889d6d5nn32WS644AIAZsyYwZQp3Q+xZ599losvvhiAKVOmMHbsWN566y2OO+44vvvd7/Kv//qvrF69mkwmw/Tp03n00UeZP38+zz33HMOGDet2Ob1p0P8C2RMZ0uQolpx4TKfMiXTXnn6Dr5aGhoa28RUrVnDzzTfzwgsvMHz4cC6++OJOz7FPpVJt4/F4nEKh81tZp9Pp3c7TGy655BKOPfZYHn74YU4//XRuu+02TjzxRBYtWsTChQuZP38+Z5xxBt/61reqVoeuRH1qadV5IkMdWVp1EFlk0NiyZQtDhw5ln332Yf369Tz22GO9Xsbxxx/Pr371KyDo6++s5dGVOXPmtJ2ttGzZMtavX8+hhx7KypUrOfTQQ7n66qs566yzWLp0KWvXrmXIkCFccsklfO1rX2PJkiW9vi3dMehbBiTryJCjNV+kIT34N1ekFsyePZvJkyczceJEDj74YI4//vheL+NLX/oSn/nMZ5g8eXLb0FUXzsc//vG2awLNmTOH2267jS984QtMmzaNZDLJHXfcQSqV4q677mLBggUkk0nGjh3Lddddx+9//3vmz59PLBYjlUrxk59Ec6vvAXMP5MbGRt+Tm9u8/l/XMHHFz1h/9VrGjWzY/QIiNWzZsmVMmjQp6mr0C4VCgUKhQCaTYcWKFZx22mmsWLGCRKL/fqnsbP+Z2WJ3b+xikTb9d6t6SSxZR9yc1mwroDAQke7Ztm0bp5xyCoVCAXfnpz/9ab8Ogr01eLcsZKkgAHLbm4FR0VZGRAaM4cOHs3jx4qir0WcG/QHkWKoOgHy26r9vExEZsAZ9GMTDMCi0NkdcExGR/mvwh0E66CbKt6plICLSlUEfBol0PQCF7PaIayIi0n8N+jBItoWBuolE+ruTTz55px+Q3XTTTVx11VW7XG7IkCEArFu3jnnz5nU6z0knncTuTk+/6aabaGnZ0Ytw5pln8uGHe39X3uuuu44bbrhhr9dTTYM/DDJBGBRz6iYS6e8uvPBC7r777nbT7r77bi688MJuLT927Ni9uupnxzBYuHAhw4cP3+P1DSQ1EAbBMYNSTt1EIv3dvHnzePjhh9tuZLNq1SrWrVvHnDlz2s77nz17NtOmTeOBBx7YaflVq1YxdepUILiM9AUXXMCkSZM455xz2L59x2fAVVdd1Xb562uvvRYIrjS6bt06Tj75ZE4++WQAxo8fz8aNGwG48cYbmTp1KlOnTm27/PWqVauYNGkSn//855kyZQqnnXZau3J2p7N1Njc384lPfKLtkta//OUvAZg/fz6TJ09m+vTpO93joTcM+t8ZpOvCH5rl1TIQ6ZFH5sO7r/buOvefBmdc3+XLI0eO5KijjuKRRx5h7ty53H333Zx33nmYGZlMhvvvv5999tmHjRs3cswxx3D22Wd3ee/fW265hfr6epYtW8bSpUvbXYL6e9/7HiNHjqRYLHLKKaewdOlSvvzlL3PjjTfy5JNPMnr06HbrWrx4MbfffjvPP/887s7RRx/Nxz72MUaMGMGKFStYsGAB//7v/855553Hvffe23bF0l3pap0rV65k7NixPPzww0BwSetNmzZx//33s3z5csysV7quOhr8LYPwmIHn1TIQGQgqu4oqu4jcnW9961tMnz6dU089lbVr1/Lee+91uZ6nn3667UN5+vTpTJ8+ve21X/3qV8yePZtZs2bx+uuv7/YidM8++yznnHMODQ0NDBkyhHPPPZdnnnkGgAkTJjBz5kxg15fJ7u46p02bxuOPP87f//3f88wzzzBs2DCGDRtGJpPhs5/9LPfddx/19fXdKqMnBn3LwJLlMNj58rYisgu7+AZfTXPnzuUrX/kKS5YsoaWlhY9+9KMA3HnnnTQ1NbF48WKSySTjx4/v9LLVu/P2229zww038OKLLzJixAguu+yyPVpPWfny1xBcArsn3USdOfzww1myZAkLFy7k29/+Nqeccgr/9E//xAsvvMATTzzBPffcw49+9CN++9vf7lU5HQ36lgHJ4Edn6iYSGRiGDBnCySefzBVXXNHuwPHmzZvZd999SSaTPPnkk7zzzju7XM+JJ57IXXfdBcBrr73G0qVLgeDy1w0NDQwbNoz33nuv7Q5jAEOHDmXr1q07rWvOnDn8+te/pqWlhebmZu6//37mzJmzV9vZ1TrXrVtHfX09F198MV//+tdZsmQJ27ZtY/PmzZx55pl8//vf55VXXtmrsjsz6FsGJDIAWEEtA5GB4sILL+Scc85pd2bRRRddxCc/+UmmTZtGY2MjEydO3OU6rrrqKi6//HImTZrEpEmT2loYM2bMYNasWUycOJGDDjqo3eWvr7zySk4//XTGjh3Lk08+2TZ99uzZXHbZZRx11FEAfO5zn2PWrFnd7hIC+O53v9t2kBhgzZo1na7zscce4+tf/zqxWIxkMsktt9zC1q1bmTt3Lq2trbg7N954Y7fL7a5BfwlrgNx1o/ndiHP5y6tv7eVaiQwuuoT1wLY3l7Ae/N1EQNbSxIrZqKshItJv1UQY5CxNvKhuIhGRrtREGOQtRUJhINItA6XrWNrb2/1WG2EQyxBXN5HIbmUyGTZt2qRAGGDcnU2bNpHJZPZ4HYP/bCKgEEuT0NlEIrs1btw41qxZQ1NTU9RVkR7KZDKMGzduj5evahiY2W3AWcAGd58aThsJ/BIYD6wCznP3D6pZj2I8QzKvloHI7iSTSSZMmBB1NSQC1e4m+gVweodp84En3P0w4InweVUV4xmSJYWBiEhXqhoG7v408H6HyXOB/wjH/wP4VDXrAEEYpDxX7WJERAasKA4g7+fu68Pxd4H9ql1gKZEhg1oGIiJdifRsIg9OWejytAUzu9LMFpnZor05oOXxOtLkKBRLe7wOEZHBLIoweM/MDgAIHzd0NaO73+ruje7eOGbMmD0vMZkhQ47WgsJARKQzUYTBg8Cl4filwM63K+ptyToyZGnNF6telIjIQFTVMDCzBcB/A0eY2Roz+yxwPfCXZrYCODV8XlWWrCNlRVqzOogsItKZqv7OwN27uov1KdUstyNLBr/Ky7a2AEP7smgRkQGhJi5HYangPsi57dsiromISP9UE2EQTwV3OytkmyOuiYhI/1RTYZBv3bt7k4qIDFa1EQbpoJso36qWgYhIZ2oiDBLpegAKuZaIayIi0j/VRBikwjAoZhUGIiKdqYkwSNYF3UQltQxERDpVG2GQKYeBDiCLiHSmJsIglQm6iTyvMBAR6UyNhEHQMnC1DEREOlUTYWDJ4HcGahmIiHSuJsKAMAysoDAQEelMbYRBPEmBOFZojbomIiL9Um2EAZAlpTAQEelC7YSBpYkX1U0kItKZmgmDvKWJF7NRV0NEpF+qnTCIpYkX1U0kItKZmgqDRElhICLSmZoJg2IsQ6KkbiIRkc7UThjE0yQVBiIinaqZMCjE60i5wkBEpDM1EwaleJqU56KuhohIv1QzYeCJDBnUMhAR6UwNhUEdaXIUiqWoqyIi0u/UTBiQyJAhR2tBYSAi0lFkYWBmXzGz183sNTNbYGaZqhaYrKPOcrTmClUtRkRkIIokDMzsQODLQKO7TwXiwAVVLTMZ3O2stbW5msWIiAxIUXYTJYA6M0sA9cC6ahZmqeCeBrntLdUsRkRkQIokDNx9LXAD8GdgPbDZ3X/TcT4zu9LMFpnZoqampr0qM9YWBmoZiIh0FFU30QhgLjABGAs0mNnFHedz91vdvdHdG8eMGbNXZcZSQTdRIaswEBHpKKpuolOBt929yd3zwH3AcdUsMF5uGbTqngYiIh1FFQZ/Bo4xs3ozM+AUYFk1C0ykGwC1DEREOhPVMYPngXuAJcCrYT1urWaZyUzQTVTM6QCyiEhHiagKdvdrgWv7qrxk2DIoZtVNJCLSUc38ArncMijl1E0kItJRzYRBqi5oGZRyahmIiHRUO2EQtgw8rzAQEemohsJgCKAwEBHpTM2EgSWD3xmgMBAR2UnNhAGJNCUMKygMREQ6qp0wMCNLCiu0Rl0TEZF+p3bCAMiRIqYwEBHZSW2FgaWJFxUGIiIddTsMzOyvzGxoOP5tM7vPzGZXr2q9LxdLE1MYiIjspCctg390961mdgLBVUd/DtxSnWpVR97SJErZqKshItLv9CQMiuHjJ4Bb3f1hINX7VaqeQjxNoqSWgYhIRz0Jg7Vm9lPgfGChmaV7uHzkCrEMSbUMRER20pMP8/OAx4CPu/uHwEjg61WpVZUU4woDEZHO9OQS1gcAD7t71sxOAqYDd1SlVlVSjGeod4WBiEhHPWkZ3AsUzexQghvRHATcVZVaVUkpniHluairISLS7/QkDEruXgDOBX7o7l8naC0MGJ7IkEYtAxGRjnoSBnkzuxD4DPBQOC3Z+1WqomQdGXIUiqWoayIi0q/0JAwuB44Fvufub5vZBOA/q1OtKklkyJCjtaAwEBGp1O0wcPc3gGuAV81sKrDG3f+lajWrhmQdCSvR2qrfGoiIVOr22UThGUT/AawCDDjIzC5196erU7XeZ8ngbmfZ7dtg2JCIayMi0n/05NTS/wWc5u5vApjZ4cAC4KPVqFg1WCq4wU2utSXimoiI9C89OWaQLAcBgLv/kQF2ADlWDoPt2yKuiYhI/9KTlsEiM/sZ8F/h84uARb1fpeqJp4JuorxaBiIi7fSkZXAV8Abw5XB4I5y2R8xsuJndY2bLzWyZmR27p+vqrkQ6CINCVmEgIlKp2y0Dd88CN4ZDb7gZeNTd55lZCqjvpfV2SWEgItK53YaBmb0KeFevu/v0nhZqZsOAE4HLwnXkgKpfJyKRaQCgmFMYiIhU6k7L4KwqlDsBaAJuN7MZwGLgandvrkJZbZJhy6CY3V7NYkREBpzdHjNw93d2NZTnM7P/7kG5CWA2cIu7zwKagfkdZzKzK81skZktampq6sHqO5fKBGFQylU1c0REBpzevDlNpgfzriH4BfPz4fN7CMKhHXe/1d0b3b1xzJgxe13BZF3wQ7NSXi0DEZFKvRkGXR5X2GlG93eB1WZ2RDjpFIKzk6oqHbYMPKcwEBGp1JPfGfS2LwF3hmcSrSS4EF5VpcIDyKhlICLSTm+GgfVkZnd/GWjsxfJ3q3xtIgq6UJ2ISKXe7Ca6pBfXVR2xGDkSWEEtAxGRSj25aulWdj4usJngkhRfc/fXerNi1dJKGlPLQESknZ50E91EcBbQXQRdQhcAhwBLgNuAk3q7ctWQsxQxhYGISDs96SY6291/6u5b3X2Lu98KfNzdfwmMqFL9el3e0sSLCgMRkUo9CYMWMzvPzGLhcB5Q/lTt9mmlUcspDEREdtKTMLiI4CDxhnC4BLjYzOqAv61C3aqiEEuTKGWjroaISL/Sk6uWrgQ+2cXLz/ZOdaqvEMuQUMtARKSdbrcMzGycmd1vZhvC4V4zG1fNylVDIZ4hqZaBiEg7Pekmuh14EBgbDv8nnDagFONpkq4wEBGp1JMwGOPut7t7IRx+Aez91eP6WCmeIa0wEBFppydhsMnMLjazeDhcDGyqVsWqpZTIkPKq30dHRGRA6UkYXAGcB7wLrAfmEd6pbCDxRB3p6t9UTURkQOl2GIQ3sznb3ce4+77u/ing01WsW1V4IkOGLIViKeqqiIj0G3t7obqv9kot+lKyjrQVaM3lo66JiEi/sbdh0KPLVvcHlqwDoHW7bn0pIlK2t2EwYC5DUVa+p0G2ZVvENRER6T92+wvkLi5dDUGroK7Xa1Rl5ZZBPtsScU1ERPqP3YaBuw/ti4r0lXg6CINcq8JARKSsN+90NiDEU8F9kPOtOmYgIlJWc2GQCFsGBXUTiYi0qcEwCA4gKwxERHaovTDIBN1ExZzCQESkrObCIJUOw0AtAxGRNjUXBslMcMyglN8ecU1ERPqPmguDdP2QYETdRCIibSINg/BS2C+Z2UN9VWY6PGbgahmIiLSJumVwNbCsLwtMZoKziVAYiIi0iSwMwvsnfwL4WZ+WG09R8BjkW/uyWBGRfi3KlsFNwDeALm8sYGZXmtkiM1vU1NTUO6WakbUUVlQYiIiURRIGZnYWsMHdF+9qPne/1d0b3b1xzJjeu91yjhRWUDeRiEhZVC2D44GzzWwVcDfwF2b2X31VeNYyxNQyEBFpE0kYuPs33X2cu48HLgB+6+4X91X5OUsRVxiIiLSJ+myiSORjGRLFbNTVEBHpN3Z7P4Nqc/engKf6ssxCLE2ipJaBiEhZTbYMgjBQy0BEpKw2wyCeIakwEBFpU5NhUIpnSLnCQESkrIbDIBd1NURE+o3aDINEhrRaBiIibWoyDDyRIY1aBiIiZTUZBiTqyJCjUChGXRMRkX6hNsMgWUfMnNasrk8kIgI1GgaWDG592drSHHFNRET6h5oOg9z2bRHXRESkf6jJMIilg7ud5bNqGYiIQI2GQTwVtgxadcxARARqNgyClkGhVd1EIiJQq2GQbgCgoLOJRESAGg2DZCboJspnWyKuiYhI/1CTYZDIBC2DksJARASo0TBIh2FQzCkMRESgRsMgmQkOIJfyOmYgIgI1GgapuiHBSE5hICICNRoGmbCbyNUyEBEBajQMkungbCIKrdFWRESkn6jJMLBYjO2egrwOIIuIQI2GAUDWUsTUMhARAWo4DHKksaLCQEQEIgoDMzvIzJ40szfM7HUzu7qv65C1NHGFgYgIAImIyi0AX3P3JWY2FFhsZo+7+xt9VYF8LKUwEBEJRdIycPf17r4kHN8KLAMO7Ms65GP1pHKb+7JIEZF+K/JjBmY2HpgFPN+X5W7b76Mcll/Ohg3v9WWxIiL9UqRhYGZDgHuBv3P3LZ28fqWZLTKzRU1NTb1a9pij/oqUFfnjM/+7V9crIjIQRRYGZpYkCII73f2+zuZx91vdvdHdG8eMGdOr5R809QQ22CgyKx7u1fWKiAxEUZ1NZMDPgWXufmMkdYjF+fN+pzB1+4t8+OH7UVRBRKTfiKplcDxwCfAXZvZyOJzZ15UYNvvTZCzP8mfu7+uiRUT6lajOJnrW3c3dp7v7zHBY2Nf1OGT2qbzPPsSWP9jXRYuI9CuRn00UpVgiwdujTmLKtj/Q0rIt6uqIiESmpsMAoG7mOTRYK8ueeSDqqoiIRKbmw+Dwo89kCw0UXlcYiEjtqvkwSKQyrBh2AhM3P0sum426OiIikaj5MABITJvLMGtm2X/rNwciUpsUBsARx82lxdO0Lv111FUREYmEwgDI1A9h+dBjOPT9pygWClFXR0SkzykMQj7pbEaxmT+++HjUVRER6XMKg9DhJ5xL1pNseanTyySJiAxqCoPQ0GEjeaO+kYM3/BYvlaKujohIn1IYVMgd/gn2ZyMrX3km6qqIiPQphUGFw+acR97jbHpR9zgQkdqiMKgwcvR+vJGZwUfWPcS6lx6NujoiIn1GYdBB6cT55D3G2AfOZ9WNp9Cy8r+jrpKISNUpDDqYdfzHif/dEn69/5do2PxH6u84nfU/mYu/+2rUVRMRqRpz96jr0C2NjY2+aNGiPi3zlT+tYem9/8LZzfcwzFr4cMKZDJ92Jow7EkYfDjFlqYj0b2a22N0bdzufwmDXSiXngT+8zgeP/y/OLT3GcGsGIJcYQm6/WdR/5Ghi446EA2bA0P3BrM/rKCLSle6GQaIvKjOQxWLGOcdNZcvsW/n1ktWs+uNSfPWLfKR1ObNWv8XEtc8RI/hdQj7SYk37AAAN20lEQVQ9Att/KokDpsN+U2D/qUELIlkX8VaIiOyaWgZ7wN1Z88F2Fr3zPi//aR1bVi5i6OblTLI/Mzn2DkfE1pAhF8yLwbBx2KhDYfRhMOrQYBh9OAwbp5aEiFSVuon62ObteV5Z/SEv/flDXnpnI++vXs5BuZUcYuv4SGwdRyQ3MN7XUectbct4qgEbMxEqh9GHwT5jIZGOcGtEZLBQGESsVHJWbmzmzXe3smLDVlZs2MZb725ly6a1/I/SOg6JreMwW8OUxFoOi61lROmDdssXMyOxffYnNvQAGHpAcDxiyL7QMBoa9oWGMcHzzHAdyBaRLumYQcRiMePQfYdw6L5DgAPapheKJd55v4U/bdjG2xubuW9jMyubmtnY9C6jWlYyPvYu+/EB+xU+YP/mDxi7YRX72UuM9A/bjk1UcovjdSOxhtFY/ShoGAX14ZAZFhyvSDZAqr7isR5SQ4LxVEMwPa4/BZFapk+APpaIxzhkzBAOGTNkp9e2tOb586YWmrZmeW9LK8u2ZnlqSyvvbcmycUsz+a0boXkT+/iHjGYzoy0YRuS3MnLrNvaNb2JU7B2Gs4Whpa2dhkdXPJ6GZB3WdgwjfDQLxtNDgnBJ7xM8lodEBuKpIEziqWCIJYJurkRdGEaZcDyc1x28CF4Kx8PHdkHVEIz3tNVTbunqWIxIjygM+pF9MkmmHjhsl/O4O1taC2zclmXj1iwbt+XY1JzljW05nt6WZdO2LJu25Xh/awv51maKuRYSxe3Uk6WeVuosSwOt1JGl3oJp9WRpKGTJZLOYBTFghL9INIhTYtj2HCNiLewTe5+hrGaIN1Nf2kbCc8S9WL03JVkfhAsWfMCXw8liQYiUilAqBOFSKgQDQCwZhlPFYywRLLfTEK6vrYzKaW1vfPt6mQXriyUgFt8xbrEdAYeHy5UDKl4xfzx8Ht8RjqXijm3y4o7trKxP5XOs/aPFwnXGdqy77THW4Xn5sXLbK8opFaGYh2IWirlwPBe8v+3qEe+ifhVD27aFj+Vxi7V//8rvD+zYl6VCxT4u7fzFo7xf233BKEGp1GEfsGM/BDsjLDvWvuzy3xXe/ssKdL6/Y/Edfx8d93l5+fK0yr+F9n9M4TpKO97vQi4YL+SC5ydeExxLrCKFwQBjZgyrSzKsLtlp66IzuUKJ5myBbeHQnC3QkivSkivQnA0fc0XezxXBnZKDEz46FEsltmWLbGnNs2V7MGwOh9ZCiWyhQNyLJCmQoECKImlyZCxHhnzFeI4kBRyjRIwisXA8+GeoI0c9rTRYK/W0MjSWZahnSZgTN0iYE4tBIhwvEiPvMXKlGDmPkS8ZOQ8+wNOlIhlKpL1IulggbUXSViQeI1hXzEkYxM1JWCf/wF7CvISbBeEYfuiWwzJmTpwScbYHj14Mt6iEhz/s9/IHdbh9Fr4e9yLmJWJent9wi1EiFj7GKYVBZF7Cyu+Slx/L04I6G4SvOebFcJ4iMS9BWJaFH5QWfhBbpx9KO3MMj6fxeBKPJYMPTRwLP3TNS0AJK1V+gO4YDA9qGAaQlx/L6ykVwjoVsFL7uwx6LNn2gevhFwIr5duCqbvbUN4O2vYjOz7g+7N4Omhhx5PQeMXgDQMzOx24GYgDP3P366Oqy2CXSsRIJVKMaEhVrYxCsUS2UCJX2PGYKxZpzZfIFcPnhRL5YomSQ7HklNzbPZaXbc0Xg8dCkQ/zJQrFErmiUyiWKJScfDFYT8yMZDxGMh4jlTBS4TjQVuaOOgXrzOaD8dYOjzEzYjEjbkY8ZsRiELfg46ZY8rah5E6h5BSLTr5UIl8Mpg88QZjFwlApDzGcGCUKxMmToEjYatiLcnqyfPmrQakbV8qJW4n6WIlUrEDRYxRKkHej6MGXjVJFEFeKGcTMiJuTtBIJc1KxEkkLvp545btiwaMBqRikYkVSMSdVsQxAEcPdKDmUAHejgOEOhZJRdCiG3zlKWPj3FnwxKY+DkfUE2VKcrMcpuuEFp5h3fsnBHNHtd3HPRBIGZhYH/g34S2AN8KKZPejub0RRH9l7iXiMRDxGQw2eEVsqVQRD0dsfbiF46kCxGARJoVSiEI4XSyXMdoRQPGYkYkEwQdhIwdt6GcottlKpvPyOMC2GQbkjMHcEaNmOVk4w3rZsRTAXio6ZtX1oxmLBeMdwzBdLFCvq0Rl3D8O1/TbGY9ZWViFcRyFcX7HkYQvMdjr0U2ybN3wfK+ZvC/K2x3D/lD+E3fFwf7W9j+6USjvGK3sDK8+0LDnBe1Tc8V4VSsGy5X1tba3IsPUY1iVmEI9Z23sabEew/mK7sr1tmXj5y0ks2Af71FX/ozqqlsFRwFvuvhLAzO4G5gIKAxlwYjEjHYuTVqerDGBRnaB+ILC64vmacFo7ZnalmS0ys0VNTU19VjkRkVrTr3+t5O63unujuzeOGTMm6uqIiAxaUYXBWuCgiufjwmkiIhKBqMLgReAwM5tgZingAuDBiOoiIlLzIjnk5e4FM/tb4DGCU0tvc/fXo6iLiIhE+DsDd18ILIyqfBER2aFfH0AWEZG+oTAQEZGBcz8DM2sC3tnDxUcDG3uxOgOFtru2aLtrT3e2/WB33+25+QMmDPaGmS3qzs0dBhttd23Rdtee3tx2dROJiIjCQEREaicMbo26AhHRdtcWbXft6bVtr4ljBiIismu10jIQEZFdUBiIiMjgDgMzO93M3jSzt8xsftT16U1mdpCZPWlmb5jZ62Z2dTh9pJk9bmYrwscR4XQzsx+E78VSM5sd7RbsHTOLm9lLZvZQ+HyCmT0fbt8vwwsgYmbp8Plb4evjo6z33jKz4WZ2j5ktN7NlZnZsLexzM/tK+Hf+mpktMLPMYNznZnabmW0ws9cqpvV4/5rZpeH8K8zs0u6UPWjDoOLWmmcAk4ELzWxytLXqVQXga+4+GTgG+GK4ffOBJ9z9MOCJ8DkE78Nh4XAlcEvfV7lXXQ0sq3j+L8D33f1Q4APgs+H0zwIfhNO/H843kN0MPOruE4EZBO/BoN7nZnYg8GWg0d2nElzc8gIG5z7/BXB6h2k92r9mNhK4Fjia4K6S15YDZJc8vPfmYBuAY4HHKp5/E/hm1PWq4vY+QHBP6TeBA8JpBwBvhuM/BS6smL9tvoE2ENz/4gngL4CHCG45uxFIdNz3BFfGPTYcT4TzWdTbsIfbPQx4u2P9B/s+Z8edEUeG+/Ah4OODdZ8D44HX9nT/AhcCP62Y3m6+roZB2zKgm7fWHAzCZvAs4HlgP3dfH770LrBfOD6Y3o+bgG8ApfD5KOBDdy+Ezyu3rW27w9c3h/MPRBOAJuD2sIvsZ2bWwCDf5+6+FrgB+DOwnmAfLqY29jn0fP/u0X4fzGFQE8xsCHAv8HfuvqXyNQ++Fgyqc4fN7Cxgg7svjrouEUgAs4Fb3H0W0MyOLgNg0O7zEcBcgjAcCzSwc1dKTajm/h3MYTDob61pZkmCILjT3e8LJ79nZgeErx8AbAinD5b343jgbDNbBdxN0FV0MzDczMr356jctrbtDl8fBmzqywr3ojXAGnd/Pnx+D0E4DPZ9firwtrs3uXseuI/g76AW9jn0fP/u0X4fzGEwqG+taWYG/BxY5u43Vrz0IFA+e+BSgmMJ5emfCc9AOAbYXNH0HDDc/ZvuPs7dxxPs09+6+0XAk8C8cLaO211+P+aF8w/Ib87u/i6w2syOCCedArzBIN/nBN1Dx5hZffh3X97uQb/PQz3dv48Bp5nZiLBVdVo4bdeiPlhS5QMxZwJ/BP4E/EPU9enlbTuBoLm4FHg5HM4k6Bt9AlgB/F9gZDi/EZxd9SfgVYIzMyLfjr18D04CHgrHPwK8ALwF/G8gHU7PhM/fCl//SNT13sttngksCvf7r4ERtbDPgX8GlgOvAf8JpAfjPgcWEBwXyRO0BD+7J/sXuCLc/reAy7tTti5HISIig7qbSEREuklhICIiCgMREVEYiIgICgMREUFhIDXEzLaFj+PN7K97ed3f6vD89725fpFqUxhILRoP9CgMKn7p2pV2YeDux/WwTiKRUhhILboemGNmL4fXyY+b2f80sxfD68J/AcDMTjKzZ8zsQYJfvGJmvzazxeG19a8Mp10P1IXruzOcVm6FWLju18zsVTM7v2LdT9mOexPcGf66FjO73oL7VCw1sxv6/N2RmrS7bzsig9F84Bp3Pwsg/FDf7O5HmlkaeM7MfhPOOxuY6u5vh8+vcPf3zawOeNHM7nX3+Wb2t+4+s5OyziX41fAMYHS4zNPha7OAKcA64DngeDNbBpwDTHR3N7Phvb71Ip1Qy0AkuHbLZ8zsZYLLgI8iuGEIwAsVQQDwZTN7BfgDwcXADmPXTgAWuHvR3d8DfgccWbHuNe5eIricyHiCyy23Aj83s3OBlr3eOpFuUBiIBNd4+ZK7zwyHCe5ebhk0t81kdhLBFTSPdfcZwEsE18HZU9mK8SLBjVoKBHenugc4C3h0L9Yv0m0KA6lFW4GhFc8fA64KLwmOmR0e3jSmo2EEt1NsMbOJBLcbLcuXl+/gGeD88LjEGOBEgoundSq8P8Uwd18IfIWge0mk6nTMQGrRUqAYdvf8guB+COOBJeFB3CbgU50s9yjwN2G//psEXUVltwJLzWyJB5fULruf4JaMrxBcZfYb7v5uGCadGQo8YGYZghbLV/dsE0V6RlctFRERdROJiIjCQEREUBiIiAgKAxERQWEgIiIoDEREBIWBiIgA/z8kOSWWQYldYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f23051fdcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log_loss over time\n",
    "\n",
    "# This gives us a very smooth log_loss plot, which is unusual. Keep that in mind.\n",
    "\n",
    "plt.plot(iters, losses_train, label='Training Loss')\n",
    "plt.plot(iters, losses_val, label='Validation Loss')\n",
    "plt.ylabel('Log_loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc=1)\n",
    "plt.title('Log_loss per Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2305158f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generalization Loss Over Time\n",
    "\n",
    "plt.plot(iters, gen_loss, label='Generalization Loss')\n",
    "plt.ylabel('Generalization Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc=4)\n",
    "plt.title('Generalization Loss per Iteration')\n",
    "plt.axhline(0, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9760166666666666\n",
      "Test Accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "# let's see how we did\n",
    "\n",
    "# Forward Propigate to get outputs on train data\n",
    "l1_train = activation(X_train.dot(W1) + b1) # Input -> Hidden 1 || activation(x * W + bias)\n",
    "output_train = softmax(l1_train.dot(W2) + b2)\n",
    "\n",
    "# Forward Propigate to get outputs on test data\n",
    "l1_test = activation(X_test.dot(W1) + b1) # Input -> Hidden 1 || activation(x * W + bias)\n",
    "output_test = softmax(l1_test.dot(W2) + b2) # Hidden 1 -> Output || Softmax Probabilites\n",
    "\n",
    "correct_train = 0\n",
    "for i in range(0, output_train.shape[0]):\n",
    "    if np.argmax(output_train[i]) == np.argmax(y_train[i]):\n",
    "        correct_train += 1\n",
    "\n",
    "correct_test = 0\n",
    "for i in range(0, output_test.shape[0]):\n",
    "    if np.argmax(output_test[i]) == np.argmax(y_test[i]):\n",
    "        correct_test += 1\n",
    "\n",
    "train_accuracy = correct_train / y_train.shape[0]\n",
    "test_accuracy = correct_test / y_test.shape[0]\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
